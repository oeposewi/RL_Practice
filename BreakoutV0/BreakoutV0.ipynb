{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\olive\\anaconda3\\envs\\bl_env\\lib\\runpy.py\", line 188, in _run_module_as_main\n",
      "    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n",
      "  File \"C:\\Users\\olive\\anaconda3\\envs\\bl_env\\lib\\runpy.py\", line 111, in _get_module_details\n",
      "    __import__(pkg_name)\n",
      "  File \"C:\\Users\\olive\\anaconda3\\envs\\bl_env\\lib\\site-packages\\atari_py\\__init__.py\", line 1, in <module>\n",
      "    from .ale_python_interface import *\n",
      "  File \"C:\\Users\\olive\\anaconda3\\envs\\bl_env\\lib\\site-packages\\atari_py\\ale_python_interface.py\", line 17, in <module>\n",
      "    ale_lib = cdll.LoadLibrary(os.path.join(os.path.dirname(__file__),\n",
      "  File \"C:\\Users\\olive\\anaconda3\\envs\\bl_env\\lib\\ctypes\\__init__.py\", line 460, in LoadLibrary\n",
      "    return self._dlltype(name)\n",
      "  File \"C:\\Users\\olive\\anaconda3\\envs\\bl_env\\lib\\ctypes\\__init__.py\", line 382, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "FileNotFoundError: Could not find module 'C:\\Users\\olive\\anaconda3\\envs\\bl_env\\lib\\site-packages\\atari_py\\ale_interface\\ale_c.dll' (or one of its dependencies). Try using the full path with constructor syntax.\n"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "!python -m atari_py.import_roms C:\\Users\\olive\\OneDrive\\Documents\\projects\\RL_Practice\\project1\\ROMS\\ROMS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym.envs.atari'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5468\\1537744192.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0menv_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Breakout-v0'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\bl_env_tf1\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(id, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bl_env_tf1\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(self, path, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Making new env: %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mspec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m         \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bl_env_tf1\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m             \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bl_env_tf1\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mmod_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\":\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mfn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bl_env_tf1\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bl_env_tf1\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bl_env_tf1\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bl_env_tf1\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gym.envs.atari'"
     ]
    }
   ],
   "source": [
    "env_name = 'Breakout-v0'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:1.0\n",
      "Episode:2 Score:0.0\n",
      "Episode:3 Score:2.0\n",
      "Episode:4 Score:3.0\n",
      "Episode:5 Score:1.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, info = env.step(action=action)\n",
    "        score +=reward\n",
    "    print('Episode:{} Score:{}'.format(episode,score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorise Environment and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_atari_env('Breakout-v0', n_envs = 16, seed=0)\n",
    "env = VecFrameStack(env, n_stack=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "log_path = './logs'\n",
    "model = A2C('CnnPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs\\A2C_2\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 273      |\n",
      "|    ep_rew_mean        | 1.42     |\n",
      "| time/                 |          |\n",
      "|    fps                | 218      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 36       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.682    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 0.272    |\n",
      "|    value_loss         | 0.259    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 275      |\n",
      "|    ep_rew_mean        | 1.44     |\n",
      "| time/                 |          |\n",
      "|    fps                | 221      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 72       |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.584    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.0336   |\n",
      "|    value_loss         | 0.11     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 300      |\n",
      "|    ep_rew_mean        | 1.94     |\n",
      "| time/                 |          |\n",
      "|    fps                | 219      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 109      |\n",
      "|    total_timesteps    | 24000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.928    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 0.0413   |\n",
      "|    value_loss         | 0.0386   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 309      |\n",
      "|    ep_rew_mean        | 2.15     |\n",
      "| time/                 |          |\n",
      "|    fps                | 215      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 148      |\n",
      "|    total_timesteps    | 32000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.736   |\n",
      "|    explained_variance | 0.793    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -0.0258  |\n",
      "|    value_loss         | 0.0908   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 310      |\n",
      "|    ep_rew_mean        | 2.13     |\n",
      "| time/                 |          |\n",
      "|    fps                | 213      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 187      |\n",
      "|    total_timesteps    | 40000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.786   |\n",
      "|    explained_variance | 0.818    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.065    |\n",
      "|    value_loss         | 0.0417   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 310      |\n",
      "|    ep_rew_mean        | 2.1      |\n",
      "| time/                 |          |\n",
      "|    fps                | 213      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 225      |\n",
      "|    total_timesteps    | 48000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.952   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.00221  |\n",
      "|    value_loss         | 0.00372  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 300      |\n",
      "|    ep_rew_mean        | 1.96     |\n",
      "| time/                 |          |\n",
      "|    fps                | 212      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 262      |\n",
      "|    total_timesteps    | 56000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.742   |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.0384   |\n",
      "|    value_loss         | 0.0121   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 304      |\n",
      "|    ep_rew_mean        | 2.03     |\n",
      "| time/                 |          |\n",
      "|    fps                | 213      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 299      |\n",
      "|    total_timesteps    | 64000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.744   |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.00788 |\n",
      "|    value_loss         | 0.0134   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 311      |\n",
      "|    ep_rew_mean        | 2.16     |\n",
      "| time/                 |          |\n",
      "|    fps                | 211      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 340      |\n",
      "|    total_timesteps    | 72000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.19    |\n",
      "|    explained_variance | 0.545    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -0.0207  |\n",
      "|    value_loss         | 0.0824   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 306      |\n",
      "|    ep_rew_mean        | 2.11     |\n",
      "| time/                 |          |\n",
      "|    fps                | 209      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 381      |\n",
      "|    total_timesteps    | 80000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -0.00504 |\n",
      "|    value_loss         | 0.0335   |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 315       |\n",
      "|    ep_rew_mean        | 2.29      |\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 421       |\n",
      "|    total_timesteps    | 88000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.14     |\n",
      "|    explained_variance | 0.921     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -0.000625 |\n",
      "|    value_loss         | 0.0331    |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 328      |\n",
      "|    ep_rew_mean        | 2.54     |\n",
      "| time/                 |          |\n",
      "|    fps                | 207      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 461      |\n",
      "|    total_timesteps    | 96000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.17    |\n",
      "|    explained_variance | 0.951    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.000488 |\n",
      "|    value_loss         | 0.0165   |\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x18083618400>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2c_path = './training/saved_models/a2c_breakout_model'\n",
    "model.save(a2c_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "model = A2C.load(a2c_path, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_atari_env('Breakout-v0', n_envs=1, seed=0)\n",
    "env = VecFrameStack(env, n_stack=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.8, 1.4000000000000001)\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_policy(model, env, n_eval_episodes=10, render=True))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6, 0.48989794855663565)\n"
     ]
    }
   ],
   "source": [
    "# compare trained policy to untrained policy\n",
    "model2 = A2C('CnnPolicy',env)\n",
    "print(evaluate_policy(model2, env, n_eval_episodes=10, render=True))\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b17ef8df479fa08e9962263173b244cc1b591c2737ad10bd19c79ce761cf93e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('bl_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
